# -*- coding: utf-8 -*-
"""RAG_Gemini1_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q9PaoGSM54hRgG94hb5KH0p4EBTLhM7s
"""

#--------------------------❌---------❌---------❌---------❌---------❌---------❌---------❌---------❌---------❌---------❌---------❌
!sudo apt -y -qq install tesseract-ocr libtesseract-dev

!sudo apt-get -y -qq install poppler-utils libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig

!pip install langchain
!pip install -U langchain-community
! pip install -q --upgrade google-generativeai langchain-google-genai chromadb pypdf

import urllib
import warnings
from pathlib import Path as p
from pprint import pprint

import pandas as pd
from langchain import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
import warnings
warnings.filterwarnings("ignore")
# restart python kernal if issues with langchain import.

from langchain_google_genai import ChatGoogleGenerativeAI

from google.colab import userdata
GOOGLE_API_KEY=userdata.get('GoogleAPIKEY')

model = ChatGoogleGenerativeAI(model="gemini-pro",google_api_key=GOOGLE_API_KEY,
                             temperature=0.2,convert_system_message_to_human=True)

pdf_loader = PyPDFLoader("/content/lion-king.pdf")
pages = pdf_loader.load_and_split()
print(pages[0].page_content)



from langchain_google_genai import GoogleGenerativeAIEmbeddings

text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
context = "\n\n".join(str(p.page_content) for p in pages)
texts = text_splitter.split_text(context)

embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001",google_api_key=GOOGLE_API_KEY)

vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={"k":5})

qa_chain = RetrievalQA.from_chain_type(
    model,
    retriever=vector_index,
    return_source_documents=True

)

question = "who is simba in this story?"
result = qa_chain({"query": question})
result["result"]

question = "who are the freinds of simba?"
result = qa_chain({"query": question})
result["result"]

question = "who is enemy of simba?"
result = qa_chain({"query": question})
result["result"]

question = "why simba ran away from his home?"
result = qa_chain({"query": question})
result["result"]







