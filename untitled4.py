# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GJremDJPkNp1DHqozk38UcrG0U8Zabdh
"""

import streamlit as st
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# Streamlit UI
st.title("ğŸ“„ AI PDF Q&A with Gemini Pro")

# Upload PDF
uploaded_file = st.file_uploader("ğŸ“‚ Upload a PDF", type=["pdf"])
GOOGLE_API_KEY = st.text_input("ğŸ”‘ Enter Google API Key", type="password")

if uploaded_file and GOOGLE_API_KEY:
    with st.spinner("ğŸ”„ Processing PDF..."):
        # Load PDF
        pdf_loader = PyPDFLoader(uploaded_file)
        pages = pdf_loader.load_and_split()

        # Prepare context
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
        context = "\n\n".join(str(p.page_content) for p in pages)
        texts = text_splitter.split_text(context)

        # Initialize Model & Embeddings
        model = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY, temperature=0.2)
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GOOGLE_API_KEY)

        # Create Vector Store
        vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={"k": 5})

        # QA Chain
        qa_chain = RetrievalQA.from_chain_type(model, retriever=vector_index, return_source_documents=True)

        # User Question
        question = st.text_input("â“ Ask a question about the PDF")

        if st.button("ğŸ” Get Answer"):
            with st.spinner("ğŸ¤– Thinking..."):
                result = qa_chain({"query": question})
                st.write("ğŸ’¬ **Answer:**", result["result"])